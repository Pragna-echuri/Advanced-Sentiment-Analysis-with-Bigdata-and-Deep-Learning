{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b43242a-f42b-4129-8a99-60edfe1c0a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 14:32:10.644645: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-02 14:32:10.648970: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-02 14:32:10.737904: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-02 14:32:10.738815: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-02 14:32:12.225724: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "25/04/02 14:32:16 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rand\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# ================================\n",
    "# 1️⃣ Initialize Spark Session\n",
    "# ================================\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SentimentAnalysis\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://master:9000\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d17e883f-f7a0-438a-8855-efc89a3e7a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 2️⃣ Load Dataset into Spark\n",
    "# ================================\n",
    "df = spark.read.csv(\"hdfs://localhost:9000/covid/twitter_dataset.csv\", header=True, inferSchema=True)\n",
    "df = df.na.drop()  # Drop missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3625f6c9-0acd-4270-a920-ff6efec8aaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+--------------------+----+--------------+-------------+---------------+--------------------+--------------------+--------------------+--------------------+--------+-----+-----+-----+---------+\n",
      "|      id|created_at|              source|       original_text|lang|favorite_count|retweet_count|original_author|            hashtags|       user_mentions|               place|         clean_tweet|compound|  neg|  neu|  pos|sentiment|\n",
      "+--------+----------+--------------------+--------------------+----+--------------+-------------+---------------+--------------------+--------------------+--------------------+--------------------+--------+-----+-----+-----+---------+\n",
      "|1.25e+18|2020-04-19|\"<a href=\"\"http:/...|RT @morethanmySLE...|  en|           0.0|        474.0|DrJeffreyPOlson|         coronavirus|       morethanmySLE|      JPO Aesthetics|break new york wo...| -0.5994|0.262|0.738|  0.0|      neg|\n",
      "|1.25e+18|2020-04-19|\"<a href=\"\"http:/...|RT @ClevelandClin...|  en|           0.0|          0.0|     IDevilNeck|             COVID19|     ClevelandClinic|        North Coast |prevent key know ...|  0.4019|  0.0|0.619|0.381|      pos|\n",
      "|1.25e+18|2020-04-19|\"<a href=\"\"http:/...|RT @OIC_IPHRC: 1/...|  en|           0.0|        227.0|kashif_afzalkpk|OIC, Islamophobic...|           OIC_IPHRC|Dubai, United Ara...|iphrc condemn unr...| -0.3818|0.271|0.729|  0.0|      neg|\n",
      "|1.25e+18|2020-04-19|\"<a href=\"\"http:/...|RT @keithgerein: ...|  en|           0.0|         88.0|       JulieCGD| abhealth, covid19ab|         keithgerein|     Alberta, Canada|covid19 alberta d...| -0.3818|0.302|0.698|  0.0|      neg|\n",
      "|1.25e+18|2020-04-19|\"<a href=\"\"http:/...|RT @DailySignal: ...|  en|           0.0|       9021.0|    CJHanselman|COVID19, Medicare...|DailySignal, SenS...|        Florida, USA|mani left say pro...|     0.0|  0.0|  1.0|  0.0|      neu|\n",
      "+--------+----------+--------------------+--------------------+----+--------------+-------------+---------------+--------------------+--------------------+--------------------+--------------------+--------+-----+-----+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- original_text: string (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- favorite_count: string (nullable = true)\n",
      " |-- retweet_count: string (nullable = true)\n",
      " |-- original_author: string (nullable = true)\n",
      " |-- hashtags: string (nullable = true)\n",
      " |-- user_mentions: string (nullable = true)\n",
      " |-- place: string (nullable = true)\n",
      " |-- clean_tweet: string (nullable = true)\n",
      " |-- compound: string (nullable = true)\n",
      " |-- neg: string (nullable = true)\n",
      " |-- neu: string (nullable = true)\n",
      " |-- pos: string (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "9609"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.show(5)\n",
    "df.printSchema()\n",
    "df.count()  # Check number of rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd23bf71-c036-48c9-a253-d01838ac1d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=============================>                            (6 + 6) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "|      pos| 3430|\n",
      "|    0.422|    1|\n",
      "|      0.0|   68|\n",
      "|      neg| 2053|\n",
      "|    0.268|    1|\n",
      "|    0.271|    1|\n",
      "|      neu| 4005|\n",
      "|      1.0|    6|\n",
      "|    0.419|    1|\n",
      "|    0.806|    1|\n",
      "|    0.219|    1|\n",
      "|    0.238|    1|\n",
      "|    0.231|    1|\n",
      "|    0.184|    1|\n",
      "|    0.894|    1|\n",
      "|    0.359|    1|\n",
      "|    0.242|    1|\n",
      "|    0.672|    1|\n",
      "|    0.177|    1|\n",
      "|    0.627|    1|\n",
      "+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.groupBy(\"sentiment\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0124ad3-8646-4aab-a3fa-0fa2c8e44830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "|      pos| 3483|\n",
      "|      neg| 2053|\n",
      "|      neu| 4073|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col, row_number, rand, min\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# 3️⃣ Fix Sentiment Column\n",
    "# ================================\n",
    "df = df.withColumn(\n",
    "    \"sentiment\",\n",
    "    when(col(\"sentiment\") == \"pos\", \"pos\")\n",
    "    .when(col(\"sentiment\") == \"neg\", \"neg\")\n",
    "    .when(col(\"sentiment\") == \"neu\", \"neu\")\n",
    "    .otherwise(\n",
    "        when(col(\"sentiment\") >= 0.05, \"pos\")\n",
    "        .when(col(\"sentiment\") <= -0.05, \"neg\")\n",
    "        .otherwise(\"neu\")\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Verify fixed sentiment categories\n",
    "df.groupBy(\"sentiment\").count().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f0dd26-4f02-44f5-8bac-59a1ccb7a586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 4️⃣ Reduce Dataset to Balanced Samples\n",
    "# ================================\n",
    "# Get minimum class count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f681161-80a5-4040-b08e-06d55ba92ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "|      pos| 3483|\n",
      "|      neg| 2053|\n",
      "|      neu| 4073|\n",
      "+---------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:=================================>                       (7 + 5) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "|      neg| 2053|\n",
      "|      neu| 2053|\n",
      "|      pos| 2053|\n",
      "+---------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Stratified sampling using window function\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# ✅ Step 1: Check the smallest class count\n",
    "sentiment_counts = df.groupBy(\"sentiment\").count()\n",
    "sentiment_counts.show()\n",
    "\n",
    "min_class_count = sentiment_counts.agg(F.min(\"count\")).collect()[0][0]\n",
    "\n",
    "# ✅ Step 2: Apply stratified sampling\n",
    "window_spec = Window.partitionBy(\"sentiment\").orderBy(F.rand())\n",
    "df = df.withColumn(\"row_num\", F.row_number().over(window_spec))\n",
    "\n",
    "# Keep only `min_class_count` samples per class\n",
    "balanced_df = df.filter(F.col(\"row_num\") <= min_class_count).drop(\"row_num\")\n",
    "\n",
    "# ✅ Step 3: Verify final distribution\n",
    "balanced_df.groupBy(\"sentiment\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d98e074e-0dd4-4d49-a2c8-ebfadf8954a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:=========>                                              (2 + 10) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "|      neg| 2053|\n",
      "|      neu| 2053|\n",
      "|      pos| 2053|\n",
      "+---------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Verify new balanced dataset count\n",
    "balanced_df.groupBy(\"sentiment\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4fbc55e-6f29-422c-8813-82c5f2a42a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  created_at                                             source  \\\n",
      "0  1.26e+18  2020-05-06  \"<a href=\"\"https://mobile.twitter.com\"\" rel=\"\"...   \n",
      "1  1.25e+18  2020-04-26  \"<a href=\"\"https://mobile.twitter.com\"\" rel=\"\"...   \n",
      "2  1.25e+18  2020-04-26  \"<a href=\"\"http://twitter.com/download/iphone\"...   \n",
      "3  1.26e+18  2020-05-22  \"<a href=\"\"https://mobile.twitter.com\"\" rel=\"\"...   \n",
      "4  1.26e+18  2020-05-12  \"<a href=\"\"http://twitter.com/download/iphone\"...   \n",
      "\n",
      "                                       original_text lang favorite_count  \\\n",
      "0  RT @nccdd: Did you miss our #COVID19 #webinars...   en            3.0   \n",
      "1  RT @AdvocateKids: Our #HealthCareHeroes are pu...   en            0.0   \n",
      "2  RT @iran_policy: #VirtualConference #Coronavir...   en            0.0   \n",
      "3  RT @UNICEF_ECA: The #COVID19 pandemic has made...   en            0.0   \n",
      "4  RT @OzraeliAvi: #BREAKING: Chinese human right...   en            0.0   \n",
      "\n",
      "  retweet_count  original_author  \\\n",
      "0           0.0  DINorthCarolina   \n",
      "1          21.0       Mfnaughton   \n",
      "2        5566.0           zsarir   \n",
      "3           0.0      Chulho_Hyun   \n",
      "4         220.0   Aurora73041613   \n",
      "\n",
      "                                            hashtags  \\\n",
      "0                                  COVID19, webinars   \n",
      "1                   HealthCareHeroes, InThisTogether   \n",
      "2  VirtualConference, Coronavirus, IranProtests, ...   \n",
      "3                                   COVID19, Georgia   \n",
      "4                                           BREAKING   \n",
      "\n",
      "               user_mentions                place  \\\n",
      "0                      nccdd     Huntersville, NC   \n",
      "1               AdvocateKids             Illinois   \n",
      "2                iran_policy        Virginia, USA   \n",
      "3  UNICEF_ECA, unicefgeorgia  Geneva, Switzerland   \n",
      "4                 OzraeliAvi            Hong Kong   \n",
      "\n",
      "                                         clean_tweet compound    neg    neu  \\\n",
      "0  miss futur plan practic health guidanc worri w...  -0.1531  0.167  0.833   \n",
      "1                       put faith fear fight covid19  -0.4588  0.547  0.189   \n",
      "2                              crisi iran regim fear  -0.4939  0.516  0.484   \n",
      "3  pandem made life difficult young peopl ask chi...  -0.3612  0.238  0.762   \n",
      "4  chines human right lawyer zhang xuezhong disap...  -0.2263  0.147  0.853   \n",
      "\n",
      "     pos sentiment  \n",
      "0    0.0       neg  \n",
      "1  0.264       neg  \n",
      "2    0.0       neg  \n",
      "3    0.0       neg  \n",
      "4    0.0       neg  \n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 5️⃣ Convert to Pandas\n",
    "# ================================\n",
    "if balanced_df.count() > 0:\n",
    "    pandas_df = balanced_df.toPandas()\n",
    "    print(pandas_df.head())\n",
    "else:\n",
    "    print(\"🚨 Error: Balanced dataset is empty! Check your filtering conditions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fcfde10-fd2b-4db0-996e-e23942298126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Original Dataset Count: 9609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Balanced Dataset Count: 6159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:===================================================>    (11 + 1) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "|      neg| 2053|\n",
      "|      neu| 2053|\n",
      "|      pos| 2053|\n",
      "+---------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 🚀 Debug: Check if the dataset is empty before conversion\n",
    "print(\"✅ Original Dataset Count:\", df.count())\n",
    "print(\"✅ Balanced Dataset Count:\", balanced_df.count())\n",
    "\n",
    "# Check class distribution after balancing\n",
    "balanced_df.groupBy(\"sentiment\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312fa500-0834-43a6-90c4-d718e5ab1909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa7cc482-4f6f-4420-9ac1-07c275e99758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/satvika/.local/lib/python3.8/site-packages (1.24.3)\n",
      "Requirement already satisfied: pandas in /home/satvika/.local/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: scikit-learn in /home/satvika/.local/lib/python3.8/site-packages (1.3.2)\n",
      "Requirement already satisfied: nltk in /home/satvika/.local/lib/python3.8/site-packages (3.9.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/satvika/.local/lib/python3.8/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/satvika/.local/lib/python3.8/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/satvika/.local/lib/python3.8/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/satvika/.local/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/satvika/.local/lib/python3.8/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/satvika/.local/lib/python3.8/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (7.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/satvika/.local/lib/python3.8/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/satvika/.local/lib/python3.8/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy pandas scikit-learn nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3df710d8-a581-4b72-8f09-0b16d0b6069e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/satvika/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Model: Naïve Bayes\n",
      "Accuracy: 0.7508\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.77      0.78      0.77       431\n",
      "    Positive       0.72      0.79      0.75       408\n",
      "     Neutral       0.77      0.67      0.72       393\n",
      "\n",
      "    accuracy                           0.75      1232\n",
      "   macro avg       0.75      0.75      0.75      1232\n",
      "weighted avg       0.75      0.75      0.75      1232\n",
      "\n",
      "\n",
      "🔹 Model: Logistic Regression\n",
      "Accuracy: 0.8239\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.90      0.79      0.84       431\n",
      "    Positive       0.89      0.76      0.82       408\n",
      "     Neutral       0.72      0.93      0.81       393\n",
      "\n",
      "    accuracy                           0.82      1232\n",
      "   macro avg       0.84      0.83      0.82      1232\n",
      "weighted avg       0.84      0.82      0.82      1232\n",
      "\n",
      "\n",
      "🔹 Model: Support Vector Machine (SVM)\n",
      "Accuracy: 0.8531\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.92      0.82      0.87       431\n",
      "    Positive       0.92      0.79      0.85       408\n",
      "     Neutral       0.75      0.96      0.84       393\n",
      "\n",
      "    accuracy                           0.85      1232\n",
      "   macro avg       0.87      0.86      0.85      1232\n",
      "weighted avg       0.87      0.85      0.85      1232\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# 1️⃣ Load your dataset\n",
    "df = pandas_df.copy()  # Ensure it contains 'clean_tweet' and 'sentiment' columns\n",
    "\n",
    "# 2️⃣ Text Preprocessing Function\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the text column\n",
    "df[\"clean_tweet\"] = df[\"clean_tweet\"].astype(str).apply(clean_text)\n",
    "\n",
    "# 3️⃣ Convert Sentiments into Numerical Labels\n",
    "sentiment_mapping = {\"pos\": 1, \"neg\": 0, \"neu\": 2}\n",
    "df[\"sentiment\"] = df[\"sentiment\"].map(sentiment_mapping)\n",
    "\n",
    "# 4️⃣ Convert Text Data into Numerical Representation (TF-IDF)\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Use 5000 important words\n",
    "X = vectorizer.fit_transform(df[\"clean_tweet\"])\n",
    "y = df[\"sentiment\"]\n",
    "\n",
    "# 5️⃣ Split Data into Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 6️⃣ Train Models\n",
    "models = {\n",
    "    \"Naïve Bayes\": MultinomialNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Support Vector Machine (SVM)\": SVC(kernel=\"linear\")\n",
    "}\n",
    "\n",
    "# Train and Evaluate\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"\\n🔹 Model: {model_name}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=[\"Negative\", \"Positive\", \"Neutral\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69401a6e-9ab8-49de-ba8d-702f37104047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.8/dist-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in /home/satvika/.local/lib/python3.8/site-packages (from xgboost) (1.24.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.8/dist-packages (from xgboost) (2.26.2)\n",
      "Requirement already satisfied: scipy in /home/satvika/.local/lib/python3.8/site-packages (from xgboost) (1.10.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6a61fd3-fa53-4164-bbb0-9c442565af77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/satvika/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Model: Naïve Bayes\n",
      "Accuracy: 0.7508\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.77      0.78      0.77       431\n",
      "    Positive       0.72      0.79      0.75       408\n",
      "     Neutral       0.77      0.67      0.72       393\n",
      "\n",
      "    accuracy                           0.75      1232\n",
      "   macro avg       0.75      0.75      0.75      1232\n",
      "weighted avg       0.75      0.75      0.75      1232\n",
      "\n",
      "Cross-validation Accuracy: 0.7350 ± 0.0037\n",
      "\n",
      "🔹 Model: Logistic Regression\n",
      "Accuracy: 0.8239\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.90      0.79      0.84       431\n",
      "    Positive       0.89      0.76      0.82       408\n",
      "     Neutral       0.72      0.93      0.81       393\n",
      "\n",
      "    accuracy                           0.82      1232\n",
      "   macro avg       0.84      0.83      0.82      1232\n",
      "weighted avg       0.84      0.82      0.82      1232\n",
      "\n",
      "Cross-validation Accuracy: 0.8172 ± 0.0159\n",
      "\n",
      "🔹 Model: Support Vector Machine (SVM)\n",
      "Accuracy: 0.8531\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.92      0.82      0.87       431\n",
      "    Positive       0.92      0.79      0.85       408\n",
      "     Neutral       0.75      0.96      0.84       393\n",
      "\n",
      "    accuracy                           0.85      1232\n",
      "   macro avg       0.87      0.86      0.85      1232\n",
      "weighted avg       0.87      0.85      0.85      1232\n",
      "\n",
      "Cross-validation Accuracy: 0.8430 ± 0.0135\n",
      "\n",
      "🔹 Model: Random Forest\n",
      "Accuracy: 0.8701\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.95      0.81      0.87       431\n",
      "    Positive       0.93      0.83      0.88       408\n",
      "     Neutral       0.77      0.98      0.86       393\n",
      "\n",
      "    accuracy                           0.87      1232\n",
      "   macro avg       0.88      0.87      0.87      1232\n",
      "weighted avg       0.89      0.87      0.87      1232\n",
      "\n",
      "Cross-validation Accuracy: 0.8552 ± 0.0157\n",
      "\n",
      "🔹 Model: Gradient Boosting\n",
      "Accuracy: 0.7857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.95      0.66      0.78       431\n",
      "    Positive       0.93      0.71      0.80       408\n",
      "     Neutral       0.64      1.00      0.78       393\n",
      "\n",
      "    accuracy                           0.79      1232\n",
      "   macro avg       0.84      0.79      0.79      1232\n",
      "weighted avg       0.84      0.79      0.79      1232\n",
      "\n",
      "Cross-validation Accuracy: 0.7800 ± 0.0205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:158: UserWarning: [14:38:11] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Model: XGBoost\n",
      "Accuracy: 0.8563\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.97      0.77      0.86       431\n",
      "    Positive       0.92      0.81      0.86       408\n",
      "     Neutral       0.74      1.00      0.85       393\n",
      "\n",
      "    accuracy                           0.86      1232\n",
      "   macro avg       0.88      0.86      0.86      1232\n",
      "weighted avg       0.88      0.86      0.86      1232\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:158: UserWarning: [14:42:53] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:158: UserWarning: [14:50:31] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:158: UserWarning: [14:58:05] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:158: UserWarning: [15:03:00] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:158: UserWarning: [15:09:03] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation Accuracy: 0.8522 ± 0.0135\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# 1️⃣ Load your dataset\n",
    "df = pandas_df.copy()  # Ensure it contains 'clean_tweet' and 'sentiment' columns\n",
    "\n",
    "# 2️⃣ Text Preprocessing Function\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the text column\n",
    "df[\"clean_tweet\"] = df[\"clean_tweet\"].astype(str).apply(clean_text)\n",
    "\n",
    "# 3️⃣ Convert Sentiments into Numerical Labels\n",
    "sentiment_mapping = {\"pos\": 1, \"neg\": 0, \"neu\": 2}\n",
    "df[\"sentiment\"] = df[\"sentiment\"].map(sentiment_mapping)\n",
    "\n",
    "# 4️⃣ Convert Text Data into Numerical Representation (TF-IDF)\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Use 5000 important words\n",
    "X = vectorizer.fit_transform(df[\"clean_tweet\"])\n",
    "y = df[\"sentiment\"]\n",
    "\n",
    "# 5️⃣ Split Data into Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 6️⃣ Define Models\n",
    "models = {\n",
    "    \"Naïve Bayes\": MultinomialNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Support Vector Machine (SVM)\": SVC(kernel=\"linear\"),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "}\n",
    "\n",
    "# Train and Evaluate Models\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\n🔹 Model: {model_name}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=[\"Negative\", \"Positive\", \"Neutral\"]))\n",
    "\n",
    "    # Cross-validation\n",
    "    scores = cross_val_score(model, X, y, cv=5)\n",
    "    print(f\"Cross-validation Accuracy: {scores.mean():.4f} ± {scores.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333db4c5-b57b-41bc-ac51-01b6b63bd9f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
